{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train: (10680, 100)\n",
      "shape of y_train: (10680,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devra\\anaconda3\\envs\\dev1\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\devra\\AppData\\Local\\Temp\\ipykernel_15180\\2803293684.py:88: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_df = pd.concat([evaluation_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance for E-I: [0.54 0.46]\n",
      "C:\\\\Users\\\\devra\\\\Downloads\\\\Codes_PhD\\models\\word2vec_smote\\BoostingPara_SMOTE_E-I.sav\n",
      "shape of X_train: (11963, 100)\n",
      "shape of y_train: (11963,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devra\\anaconda3\\envs\\dev1\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance for N-S: [0.78 0.22]\n",
      "C:\\\\Users\\\\devra\\\\Downloads\\\\Codes_PhD\\models\\word2vec_smote\\BoostingPara_SMOTE_N-S.sav\n",
      "shape of X_train: (7508, 100)\n",
      "shape of y_train: (7508,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devra\\anaconda3\\envs\\dev1\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance for F-T: [0.56 0.44]\n",
      "C:\\\\Users\\\\devra\\\\Downloads\\\\Codes_PhD\\models\\word2vec_smote\\BoostingPara_SMOTE_F-T.sav\n",
      "shape of X_train: (8384, 100)\n",
      "shape of y_train: (8384,)\n",
      "Feature Importance for J-P: [0.56 0.44]\n",
      "C:\\\\Users\\\\devra\\\\Downloads\\\\Codes_PhD\\models\\word2vec_smote\\BoostingPara_SMOTE_J-P.sav\n",
      "  Target  Accuracy  Precision    Recall  F1-Score   Roc-AUC\n",
      "0    E-I  0.696255   0.716406  0.650449  0.681836  0.696289\n",
      "1    N-S  0.773320   0.722137  0.911212  0.805731  0.768797\n",
      "2    F-T  0.570820   0.560806  0.822630  0.666942  0.559030\n",
      "3    J-P  0.583015   0.633588  0.395615  0.487089  0.583194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devra\\anaconda3\\envs\\dev1\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ROOT = r\"C:\\\\Users\\\\devra\\\\Downloads\\\\Codes_PhD\"\n",
    "DATA_DIR = rf\"{ROOT}\\\\dataset\"\n",
    "MBTI_RAW_CSV_PATH = os.path.join(DATA_DIR, \"mbti_clean_biTri.csv\")\n",
    "MODEL = os.path.join(ROOT, \"models\", \"word2vec_smote\")\n",
    "\n",
    "data = pd.read_csv(MBTI_RAW_CSV_PATH)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for target_name in [\"E-I\", \"N-S\", \"F-T\", \"J-P\"]:\n",
    "    encoded_target = label_encoder.fit_transform(data[target_name])\n",
    "    data[f\"type_{target_name[0]}\"] = encoded_target\n",
    "\n",
    "training_data = data[[\"cleaned_post\", \"E-I\", \"N-S\", \"F-T\", \"J-P\"]].copy()\n",
    "\n",
    "def make_dummies(data, columns=[\"E-I\", \"N-S\", \"F-T\", \"J-P\"]):\n",
    "    for column in columns:\n",
    "        temp_dummy = pd.get_dummies(data[column], prefix=\"type\")\n",
    "        data = data.join(temp_dummy)\n",
    "    return data\n",
    "\n",
    "training_data = make_dummies(training_data)\n",
    "\n",
    "X = training_data[[\"cleaned_post\"]]\n",
    "y = training_data.drop(columns=[\"cleaned_post\"])\n",
    "\n",
    "tokenized_posts = X[\"cleaned_post\"].apply(lambda x: x.split())\n",
    "word2vec_model = Word2Vec(sentences=tokenized_posts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "y_columns = [\"E-I\", \"N-S\", \"F-T\", \"J-P\"]\n",
    "evaluation_df = pd.DataFrame(columns=[\"Target\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Roc-AUC\"])\n",
    "\n",
    "for target_name in y_columns:\n",
    "    y_target = y[f\"type_{target_name[0]}\"]\n",
    "    X_transformed = word2vec_model.wv[X[\"cleaned_post\"].apply(lambda x: x.split()).sum()]\n",
    "    X_target = X_transformed[y_target.index]\n",
    "    # Using Word2Vec for vectorization\n",
    "    X_df = pd.DataFrame(X_target, index=X.index)\n",
    "\n",
    "    # Using SMOTE for oversampling\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_over, y_over = smote.fit_resample(X_df.loc[y_target.index], y_target)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
    "\n",
    "    print('shape of X_train:',X_train.shape)\n",
    "    print('shape of y_train:',y_train.shape)\n",
    "\n",
    "    base_models = [(name, pickle.load(open(f'{MODEL}\\{name}_{target_name}.sav', 'rb'))) for name in [\"RandomForest\", \"Xgboost\"]]\n",
    "    voting_clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "    # Fit the VotingClassifier\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get the predictions for the test data\n",
    "    voting_pred_proba_test = voting_clf.predict_proba(X_test)\n",
    "\n",
    "    # Use the Voting predictions as features for AdaBoost\n",
    "    ada_boost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "    ada_boost_clf.fit(voting_pred_proba_test, y_test)\n",
    "\n",
    "    # Get feature importance from the base estimators in the ensemble\n",
    "    base_estimator_feature_importance = np.mean([estimator.feature_importances_ for estimator in ada_boost_clf.estimators_], axis=0)\n",
    "    print(f\"Feature Importance for {target_name}:\", base_estimator_feature_importance)\n",
    "\n",
    "    # Evaluate the ensemble\n",
    "    boosting_pred = ada_boost_clf.predict(voting_pred_proba_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, boosting_pred)\n",
    "    precision = metrics.precision_score(y_test, boosting_pred)\n",
    "    recall = metrics.recall_score(y_test, boosting_pred)\n",
    "    f1_score = metrics.f1_score(y_test, boosting_pred)\n",
    "    roc_auc_score = metrics.roc_auc_score(y_test, boosting_pred)\n",
    "\n",
    "    # Update the evaluation_df DataFrame\n",
    "    evaluation_df = pd.concat([evaluation_df, pd.DataFrame({\n",
    "        \"Target\": [target_name],\n",
    "        \"Accuracy\": [accuracy],\n",
    "        \"Precision\": [precision],\n",
    "        \"Recall\": [recall],\n",
    "        \"F1-Score\": [f1_score],\n",
    "        \"Roc-AUC\": [roc_auc_score]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Save the Boosting Classifier\n",
    "    boosting_filename = f'{MODEL}\\BoostingPara_SMOTE_{target_name}.sav'\n",
    "    print(boosting_filename)\n",
    "    pickle.dump(ada_boost_clf, open(boosting_filename, 'wb'))\n",
    "\n",
    "# Save the evaluation_df to a CSV file\n",
    "evaluation_df.to_csv(os.path.join(MODEL, 'evaluation_boostingPara_SMOTE1.csv'), index=False)\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
